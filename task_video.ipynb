{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('Assignment Data.xlsx')\n",
    "df.columns = ['Performance', 'VideoURL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Performance                                           VideoURL\n",
      "0       1.106000  https://fgimagestorage.blob.core.windows.net/f...\n",
      "1       2.244700  https://fgimagestorage.blob.core.windows.net/f...\n",
      "2       2.012600  https://fgimagestorage.blob.core.windows.net/f...\n",
      "3       1.770800  https://fgimagestorage.blob.core.windows.net/f...\n",
      "4       0.629300  https://fgimagestorage.blob.core.windows.net/f...\n",
      "..           ...                                                ...\n",
      "263     1.560931  https://fgimagestorage.blob.core.windows.net/f...\n",
      "264     0.948489  https://fgimagestorage.blob.core.windows.net/f...\n",
      "265     1.274918  https://fgimagestorage.blob.core.windows.net/f...\n",
      "266     0.156167  https://fgimagestorage.blob.core.windows.net/f...\n",
      "267     1.444243  https://fgimagestorage.blob.core.windows.net/f...\n",
      "\n",
      "[268 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python opencv-python-headless numpy scikit-learn tensorflow mtcnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_dir, interval=30):\n",
    "    \"\"\"\n",
    "    Extract frames from a video at a given interval.\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        output_dir (str): Directory to save extracted frames.\n",
    "        interval (int): Frame interval for extraction (default is every 30 frames).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    while success:\n",
    "        if frame_count % interval == 0:\n",
    "            frame_filename = os.path.join(output_dir, f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "        success, frame = cap.read()\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Example usage\n",
    "extract_frames(\"sample_video.mp4\", \"output_frames\", interval=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "def detect_faces(image_path):\n",
    "    \"\"\"\n",
    "    Detect faces in an image using MTCNN.\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "    Returns:\n",
    "        List of cropped face images.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    faces = detector.detect_faces(image_rgb)\n",
    "    cropped_faces = []\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, width, height = face['box']\n",
    "        cropped_face = image_rgb[y:y+height, x:x+width]\n",
    "        cropped_faces.append(cropped_face)\n",
    "\n",
    "    return cropped_faces\n",
    "\n",
    "# Example usage\n",
    "faces = detect_faces(\"output_frames/frame_0.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load FaceNet model\n",
    "facenet_model = load_model('facenet_keras.h5')  # Download model from a trusted source\n",
    "\n",
    "def preprocess_face(face_image, target_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Preprocess the face image for FaceNet.\n",
    "    Args:\n",
    "        face_image (numpy array): Cropped face image.\n",
    "        target_size (tuple): Target size for resizing (default is (160, 160)).\n",
    "    Returns:\n",
    "        Preprocessed face image as a NumPy array.\n",
    "    \"\"\"\n",
    "    face_image = cv2.resize(face_image, target_size)\n",
    "    face_image = face_image.astype('float32') / 255.0\n",
    "    mean, std = face_image.mean(), face_image.std()\n",
    "    face_image = (face_image - mean) / std\n",
    "    return np.expand_dims(face_image, axis=0)\n",
    "\n",
    "def generate_embedding(face_image):\n",
    "    \"\"\"\n",
    "    Generate a 128-dimensional face embedding using FaceNet.\n",
    "    Args:\n",
    "        face_image (numpy array): Cropped face image.\n",
    "    Returns:\n",
    "        Face embedding as a NumPy array.\n",
    "    \"\"\"\n",
    "    preprocessed_face = preprocess_face(face_image)\n",
    "    embedding = facenet_model.predict(preprocessed_face)\n",
    "    return embedding[0]\n",
    "\n",
    "# Example usage\n",
    "embedding = generate_embedding(faces[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def cluster_faces(embeddings, eps=0.5, min_samples=2):\n",
    "    \"\"\"\n",
    "    Cluster face embeddings using DBSCAN.\n",
    "    Args:\n",
    "        embeddings (list): List of face embeddings.\n",
    "        eps (float): Maximum distance between samples for clustering (default 0.5).\n",
    "        min_samples (int): Minimum samples to form a cluster (default 2).\n",
    "    Returns:\n",
    "        List of cluster labels for each embedding.\n",
    "    \"\"\"\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "    cluster_labels = db.fit_predict(embeddings_array)\n",
    "    return cluster_labels\n",
    "\n",
    "# Example usage\n",
    "embeddings = [generate_embedding(face) for face in faces]\n",
    "cluster_labels = cluster_faces(embeddings)\n",
    "print(f\"Cluster labels: {cluster_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data after face detection & mapping\n",
    "influencer_data = [\n",
    "    {\"Influencer\": \"Influencer_1\", \"Performance\": [1.106, 2.344, 1.876]},\n",
    "    {\"Influencer\": \"Influencer_2\", \"Performance\": [1.500, 1.700]},\n",
    "]\n",
    "df_influencer = pd.DataFrame(influencer_data)\n",
    "\n",
    "# Calculate average performance\n",
    "df_influencer['AveragePerformance'] = df_influencer['Performance'].apply(lambda x: sum(x) / len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Influencer            Performance  AveragePerformance\n",
      "0  Influencer_1  [1.106, 2.344, 1.876]            1.775333\n",
      "1  Influencer_2             [1.5, 1.7]            1.600000\n"
     ]
    }
   ],
   "source": [
    "print(df_influencer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Influencer</th><th>Face</th><th>Average Performance</th></tr><tr><td>Influencer_1</td><td><img src='path_to_image1' width=50></td><td>1.75</td></tr><tr><td>Influencer_2</td><td><img src='path_to_image2' width=50></td><td>1.6</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Mock data for display\n",
    "influencers = [{\"Name\": \"Influencer_1\", \"Average Performance\": 1.75, \"Face\": \"path_to_image1\"},\n",
    "               {\"Name\": \"Influencer_2\", \"Average Performance\": 1.6, \"Face\": \"path_to_image2\"}]\n",
    "\n",
    "html = \"<table><tr><th>Influencer</th><th>Face</th><th>Average Performance</th></tr>\"\n",
    "for i in influencers:\n",
    "    html += f\"<tr><td>{i['Name']}</td><td><img src='{i['Face']}' width=50></td><td>{i['Average Performance']}</td></tr>\"\n",
    "html += \"</table>\"\n",
    "\n",
    "display(HTML(html))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
